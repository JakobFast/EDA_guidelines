{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is an essential step in the data analysis process, allowing you to understand your data, identify patterns, and make informed decisions. Here is a general outline of the steps to perform EDA in Python using Pandas, Matplotlib, and Seaborn:\n",
    "\n",
    "1. Import libraries and load data\n",
    "2. Inspect data structure and summary statistics\n",
    "3. Handle missing and duplicate data\n",
    "4. Perform data type conversions if necessary\n",
    "5. Analyze distributions of numerical variables\n",
    "6. Analyze relationships between numerical variables\n",
    "7. Analyze categorical variables\n",
    "8. Analyze relationships between categorical and numerical variables\n",
    "9. Perform feature engineering if necessary\n",
    "10. Save the cleaned and processed data for further analysis or modeling\n",
    "\n",
    "Once you have a general understanding of these steps, you can dive deeper into each step with examples and explanations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import libraries and load data\n",
    "\n",
    "1.1. Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Load your dataset (in this example, we'll use the famous Iris dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace the URL with the path to your local CSV file\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "column_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect data structure and summary statistics\n",
    "\n",
    "2.1. Display the first few rows of the dataset to get an overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width      species\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Examine the dataset's structure, including the number of rows, columns, and data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Generate summary statistics for numerical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    150.000000   150.000000    150.000000   150.000000\n",
      "mean       5.843333     3.054000      3.758667     1.198667\n",
      "std        0.828066     0.433594      1.764420     0.763161\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.350000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Examine the distribution of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-setosa        50\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     50\n",
      "Name: species, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"species\"].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These initial steps will give you a basic understanding of the dataset's structure, the type of data it contains, and the overall distribution of its features. From here, you can proceed with further EDA steps such as handling missing and duplicate data, analyzing distributions and relationships between variables, and performing feature engineering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Handle missing and duplicate data\n",
    "\n",
    "3.1. Check for missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "species         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. If there are missing values, you can decide to either drop or fill them, depending on the nature of the data and the amount of missing information. Here are examples for both methods:\n",
    "\n",
    "- Drop missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill missing values (e.g., with the mean, median, or mode):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name' with the name of the column containing missing values\n",
    "mean_value = df['column_name'].mean()\n",
    "df['column_name'].fillna(mean_value, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Check for duplicate rows in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4. If there are duplicate rows, you can decide whether to keep or remove them. To remove duplicates, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps ensure that your dataset is clean and free of missing or duplicate data, allowing for more accurate analysis and modeling. Keep in mind that handling missing data is highly dependent on the context, and different strategies might be appropriate for different datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Perform data type conversions if necessary\n",
    "\n",
    "4.1. Review the data types of each column to ensure they are appropriate for the data they contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length    float64\n",
      "sepal_width     float64\n",
      "petal_length    float64\n",
      "petal_width     float64\n",
      "species          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. If you find a column with an incorrect data type, convert it to the appropriate data type using the astype() method or pd.to_datetime() for date columns. Here are examples for both:\n",
    "\n",
    "- Convert a column to a different data type (e.g., from float to int):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace 'column_name' with the name of the column to convert\n",
    "df['column_name'] = df['column_name'].astype(int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert a column to a datetime data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name' with the name of the date column\n",
    "df['column_name'] = pd.to_datetime(df['column_name'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Confirm that the data types have been updated correctly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that your dataset has the correct data types for each column is important, as it can impact subsequent analyses and visualizations. Some operations and functions are only applicable to specific data types, so it's essential to have the correct data types for accurate results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze distributions of numerical variables\n",
    "\n",
    "5.1. Use histograms to visualize the distribution of numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace 'column_name' with the name of the numerical column you want to analyze\n",
    "sns.histplot(data=df, x='column_name')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Use box plots to visualize the distribution and identify outliers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace 'column_name' with the name of the numerical column you want to analyze\n",
    "sns.boxplot(data=df, x='column_name')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Use kernel density estimation (KDE) plots to visualize the probability density of numerical variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace 'column_name' with the name of the numerical column you want to analyze\n",
    "sns.kdeplot(data=df, x='column_name')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. For datasets with multiple numerical columns, you can use pair plots to visualize the distributions and relationships between variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the distributions of numerical variables helps you understand the overall shape, central tendency, and dispersion of the data. Additionally, it allows you to identify potential outliers or issues with the data that may require further investigation or preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze relationships between numerical variables\n",
    "\n",
    "6.1. Use scatter plots to visualize the relationship between two numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name1' and 'column_name2' with the names of the numerical columns you want to analyze\n",
    "sns.scatterplot(data=df, x='column_name1', y='column_name2')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2. Use a heatmap to visualize the correlation matrix of numerical variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap to visualize the correlations\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", square=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3. Use a pair plot with regression lines to visualize relationships between multiple numerical variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, kind='reg')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4. Use a scatter plot with a color-coded categorical variable to visualize the relationship between two numerical variables, separated by a categorical variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name1', 'column_name2', and 'category_column' with the appropriate column names\n",
    "sns.scatterplot(data=df, x='column_name1', y='column_name2', hue='category_column')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the relationships between numerical variables helps you identify trends, patterns, and potential dependencies between the features. Understanding these relationships can provide valuable insights for further data analysis and modeling tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze categorical variables\n",
    "\n",
    "7.1. Use bar plots to visualize the distribution of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'category_column' with the name of the categorical column you want to analyze\n",
    "sns.countplot(data=df, x='category_column')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2. Use pie charts to visualize the distribution of categorical variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'category_column' with the name of the categorical column you want to analyze\n",
    "category_counts = df['category_column'].value_counts()\n",
    "category_counts.plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that the pie chart is circular\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3. Use a crosstab or pivot table to analyze relationships between two categorical variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'category_column1' and 'category_column2' with the names of the categorical columns you want to analyze\n",
    "crosstab = pd.crosstab(df['category_column1'], df['category_column2'])\n",
    "print(crosstab)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the distributions and relationships of categorical variables helps you understand the patterns and trends in your dataset. It also allows you to identify potential imbalances in the data, which can be important for classification tasks or when dealing with imbalanced datasets.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze relationships between categorical and numerical variables\n",
    "\n",
    "8.1. Use box plots to visualize the distribution of a numerical variable for each category of a categorical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'numerical_column' and 'category_column' with the appropriate column names\n",
    "sns.boxplot(data=df, x='category_column', y='numerical_column')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2. Use violin plots to visualize the distribution of a numerical variable for each category of a categorical variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'numerical_column' and 'category_column' with the appropriate column names\n",
    "sns.violinplot(data=df, x='category_column', y='numerical_column')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.3. Use swarm plots to visualize the distribution of a numerical variable for each category of a categorical variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'numerical_column' and 'category_column' with the appropriate column names\n",
    "sns.swarmplot(data=df, x='category_column', y='numerical_column')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.4. Use bar plots to visualize the mean, median, or another summary statistic of a numerical variable for each category of a categorical variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'numerical_column' and 'category_column' with the appropriate column names\n",
    "# You can use other aggregation functions like 'median', 'min', 'max', etc.\n",
    "mean_by_category = df.groupby('category_column')['numerical_column'].mean()\n",
    "mean_by_category.plot.bar()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the relationships between categorical and numerical variables helps you identify trends, patterns, and potential dependencies between features. Understanding these relationships can provide valuable insights for further data analysis, feature engineering, and modeling tasks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Perform feature engineering if necessary\n",
    "\n",
    "Feature engineering is the process of creating new features or modifying existing ones to improve the performance of machine learning models. This step is highly dependent on the specific dataset and problem you're working on. Below are some general examples of feature engineering techniques:\n",
    "\n",
    "9.1. Create new features by combining existing ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name1' and 'column_name2' with the appropriate column names\n",
    "df['new_feature'] = df['column_name1'] * df['column_name2']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2. Apply mathematical transformations to numerical features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name' with the appropriate column name\n",
    "df['log_transformed'] = np.log(df['column_name'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3. Convert categorical features to numerical representations:\n",
    "\n",
    "- One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'category_column' with the name of the categorical column you want to encode\n",
    "encoded_features = pd.get_dummies(df['category_column'], prefix='category_column')\n",
    "df = pd.concat([df, encoded_features], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ordinal encoding (if there's a natural order to the categories):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'category_column' with the name of the categorical column you want to encode\n",
    "# Replace 'category_order' with a list of the categories in their correct order\n",
    "category_order = ['category1', 'category2', 'category3']\n",
    "df['ordinal_encoded'] = df['category_column'].apply(lambda x: category_order.index(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.4. Normalize or standardize numerical features:\n",
    "\n",
    "- Min-max scaling (normalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name' with the name of the numerical column you want to normalize\n",
    "min_value = df['column_name'].min()\n",
    "max_value = df['column_name'].max()\n",
    "df['normalized'] = (df['column_name'] - min_value) / (max_value - min_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Z-score standardization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'column_name' with the name of the numerical column you want to standardize\n",
    "mean_value = df['column_name'].mean()\n",
    "std_value = df['column_name'].std()\n",
    "df['standardized'] = (df['column_name'] - mean_value) / std_value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that feature engineering requires domain knowledge and a deep understanding of the problem you're trying to solve. The examples provided above are just a few common techniques; the specific methods you choose should be tailored to your dataset and objectives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Reassess distributions and relationships after preprocessing and feature engineering\n",
    "\n",
    "After cleaning the data, handling missing values, performing data type conversions, and engineering new features, it's essential to reassess the distributions and relationships of the variables in your dataset.\n",
    "\n",
    "10.1. Re-run the visualization and analysis techniques from steps 5, 6, and 7 to assess the updated distributions of numerical and categorical variables and the relationships between them.\n",
    "\n",
    "10.2. Compare the updated visualizations with the initial ones to observe the effects of the preprocessing and feature engineering steps. This comparison can help you understand whether the changes have improved the quality of the data, eliminated issues or biases, and made the dataset more suitable for modeling.\n",
    "\n",
    "10.3. Based on your reassessment, decide if further preprocessing or feature engineering is necessary, and iterate through steps 3 to 9 as needed. The goal is to create a dataset that is clean, well-structured, and representative of the problem you're trying to solve, which will ultimately lead to more accurate and reliable models.\n",
    "\n",
    "Once you've completed this final step, you'll have a solid understanding of your dataset, its structure, and its underlying patterns. This understanding will help you make informed decisions about which machine learning algorithms to use, how to split your dataset for training and testing, and how to fine-tune your models for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
